# WRO-2025-Oaklabs
Future Engineers 2025 Oaklabs Github Repository

The designed solution for the WRO 2025 Future Engineers competition, developed by our team Oaklabs, is a robust and highly efficient system that integrates simplicity with technical innovation. The architecture of our robot leverages a combination of electromechanical subsystems, carefully orchestrated to maximize autonomous performance and ensure precise task execution across all stages of the challenge. Each component has been selected and configured for optimal functionality and reliability under competitive conditions.

At the core of the system are two primary microcontrollers — an Arduino Uno and a Raspberry Pi 5 — configured to operate in a distributed control architecture. The division of responsibilities between these controllers ensures that computational loads are balanced, with low-latency actuation managed by the Arduino and high-level decision logic handled by the Raspberry Pi. This approach minimizes processing bottlenecks and allows concurrent task execution, increasing overall efficiency.

The Arduino subsystem is dedicated to real-time actuation and boundary sensing. A motor driver shield is mounted on the Arduino, interfacing directly with a brushed DC motor and a high-torque servo motor to achieve precise locomotion and angular correction. Two HC-SR04 ultrasonic range sensors are connected to the Arduino’s digital I/O pins, continuously sampling boundary distances at a high polling rate. The data from these sensors is processed in a lightweight algorithm to maintain a safe offset from the field periphery, ensuring reliable path adherence.

The Raspberry Pi 5 subsystem, equipped with a wide-angle camera module and an auxiliary ultrasonic sensor, performs higher-order computation and environment mapping. The camera provides live visual input for advanced computer vision tasks, including color segmentation, block recognition, and line pattern detection. The onboard ultrasonic sensor adds redundancy to the obstacle detection system, providing corroborative data to enhance robustness. The Raspberry Pi executes these algorithms under a Linux-based OS with optimized Python routines and OpenCV libraries for image processing.

A notable innovation in our design is the implementation of a synchronous, variable-based serial communication protocol over a wired interface between the Raspberry Pi and the Arduino. This protocol enables deterministic transmission of control variables with minimal latency, ensuring that high-level perception data translates seamlessly into actuator commands.

The operational logic is structured around a two-parameter decision framework. The Raspberry Pi continuously evaluates the proximity of obstacles using its ultrasonic sensor. Upon detecting an object within a 5 cm threshold, the camera module initiates a color recognition routine to classify the object as either a red or green block. If the block is red, the Raspberry Pi transmits the control variable ‘R’ via serial link to the Arduino, instructing it to rotate the servo motor 50 degrees to the right to circumvent the obstacle. Conversely, if the block is green, the variable ‘L’ is sent, commanding a 50-degree left turn. This conditional routing ensures the robot can dynamically navigate around obstructions with high positional accuracy.

For the open challenge phase, where static obstacles are absent, the navigation logic shifts to rely on floor line pattern detection. The camera scans for specific sequences of blue and orange lines. A blue–orange sequence is interpreted as a directive to rotate clockwise, prompting the Raspberry Pi to transmit the variable ‘C’ to the Arduino. An orange–blue sequence signals a counterclockwise rotation, communicated via the variable ‘A.’ This line-based navigation strategy eliminates unnecessary sensor queries, optimizing computational efficiency during this phase.

In the final parallel parking segment, an internal counter algorithm in the Raspberry Pi monitors completed laps by incrementing counters on each detection of blue and orange lines. Upon reaching a count of twelve blue and twelve orange detections — corresponding to three full laps — the Raspberry Pi primes the system for parking mode. When it next detects a pink field marking, it transmits the variable ‘P’ to the Arduino, triggering a pre-programmed maneuver sequence to execute precise parallel parking into the designated area.

The design exemplifies a well-engineered integration of distributed control, modular sensing, and efficient communication. The use of dual microcontrollers allows computationally intensive vision and logic tasks to run in parallel with time-critical actuation and feedback loops, resulting in a responsive and adaptive robot. Our methodology ensures that the electromechanical subsystems operate harmoniously, maintaining both agility and stability across diverse field conditions.

In conclusion, our team’s solution reflects a high level of technical rigor combined with practical simplicity. The careful segregation of processing responsibilities, efficient data transmission mechanisms, and intelligent use of sensor fusion demonstrate our commitment to delivering a solution that is both innovative and reliable. This design not only fulfills the functional requirements of the WRO 2025 Future Engineers competition but also illustrates the potential of systematic engineering to solve complex autonomous navigation challenges effectively. With this approach, Oaklabs is well-positioned to achieve competitive success by demonstrating superior design, execution, and technical competence.
